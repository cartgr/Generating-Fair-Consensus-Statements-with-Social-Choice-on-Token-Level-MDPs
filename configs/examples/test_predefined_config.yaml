# Test Config for Predefined Statement Method
experiment_name: test_predefined
seed: 42
num_seeds: 2 # Not strictly needed for predefined, but kept for consistency

# Models (Evaluation model is still needed)
models:
  generation_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo" # Ignored by 'predefined' but required by Experiment structure
  # evaluation_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

# Simple test scenario
scenario:
  issue: "Should we build a new bridge across the river?"
  agent_opinions:
    Agent 1: "I support building a new bridge across the river to reduce traffic congestion, but we should ensure it includes dedicated lanes for public transportation and pedestrian walkways."
    Agent 2: "I'm open to a new bridge if we use sustainable materials, minimize environmental impact, and implement reasonable tolls to offset construction costs."

# Only run the predefined method
methods_to_run:
  - predefined
  - best_of_n

# Method parameters for Best-of-N
best_of_n:
  n: [1,2] # Number of candidates to generate and compare
  max_tokens: 150 # Max length for each candidate statement
  temperature: 1 # Sampling temperature for diversity in candidates
  beta: 1.0 # Reward scaling factor
  log_level: WARNING # Control logging level (e.g., DEBUG, INFO, WARNING)
  api_delay: 0.5 # Delay between API calls (seconds) to avoid rate limits


# Method parameters for Predefined
predefined:
  # The statement to be evaluated
  predefined_statement: "A new bridge focusing on sustainability and public access should be considered, funded partially by tolls."
  # Add any other parameters if needed, though this method doesn't use them for generation

# Output directory
output_dir: "results/" 